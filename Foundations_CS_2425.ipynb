{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO64bqbYJK77ZhzZojMIl0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescaNegriUniMiB/focsproject/blob/main/Foundations_CS_2425.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOUNDATIONS OF COMPUTER SCIENCE - PROJECT FOR A.Y. 2024-2025"
      ],
      "metadata": {
        "id": "VdV-pwd4cce3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project text\n",
        "\n",
        "1. Extract all trips with `trip_distance` larger than 50\n",
        "2. Extract all trips where `payment_type` is missing\n",
        "3. For each (`PULocationID`, `DOLocationID`) pair, determine the number of trips\n",
        "4. Save all rows with missing `VendorID`, `passenger_count`, `store_and_fwd_flag`, `payment_type` in a new DataFrame called `bad`, and remove those rows from the original DataFrame\n",
        "5. Add a `duration` column storing how long each trip has taken (use `tpep_pickup_datetime`, `tpep_dropoff_datetime`)\n",
        "6. For each pickup location, determine how many trips have started there\n",
        "7. Cluster the pickup time of the day into 30-minute intervals (e.g., from 02:00 to 02:30)\n",
        "8. For each interval, determine the average number of passengers and the average fare amount\n",
        "9. For each payment type and each interval, determine the average fare amount\n",
        "10. For each payment type, determine the interval when the average fare amount is maximum\n",
        "11. For each payment type, determine the interval when the overall ratio between the tip and the fare amounts is maximum\n",
        "12. Find the location with the highest average fare amount\n",
        "13. Build a new DataFrame (called `common`) where, for each pickup location, we keep all trips to the 5 most common destinations (i.e., each pickup location can have different common destinations)\n",
        "14. On the `common` DataFrame, for each payment type and each interval, determine the average fare amount\n",
        "15. Compute the difference of the average fare amount computed in the previous point with those computed at point 9\n",
        "16. Compute the ratio between the differences computed in the previous point and those computed in point 9  \n",
        "    **Note:** You have to compute a ratio for each pair (`payment type`, `interval`)\n",
        "17. Build chains of trips. Two trips are consecutive in a chain if:\n",
        "    1. They have the same `VendorID`\n",
        "    2. The pickup location of the second trip is also the dropoff location of the first trip\n",
        "    3. The pickup time of the second trip is after the dropoff time of the first trip\n",
        "    4. The pickup time of the second trip is at most 2 minutes later than the dropoff time of the first trip"
      ],
      "metadata": {
        "id": "uLuY7UiwceG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing\n"
      ],
      "metadata": {
        "id": "UQqQkpEHcjxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time as t\n",
        "import numpy as np\n",
        "from bisect import bisect_left, bisect_right\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "if 'executed' not in globals():\n",
        "\n",
        "  executed = True\n",
        "\n",
        "  !pip install -q gdown\n",
        "  !gdown --fuzzy https://drive.google.com/file/d/1IUOdTOYgjco0ggTVsNluQOl-xPbMZ3Z-/view?usp=sharing\n",
        "\n",
        "  df = pd.read_csv('/content/focs_data.csv',dtype={'store_and_fwd_flag':object})\n",
        "  display(df.head())\n",
        "\n",
        "  df_backup = df\n",
        "else:\n",
        "  df = df_backup\n",
        "  print(\"Reset df: ok.\")"
      ],
      "metadata": {
        "id": "r9fT-xIBcq1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRE-PROCESSING**\n",
        "The following rows are being removed because they can't represent a valid trip.\n",
        "A valid trip is assumed to have these columns always positive:\n",
        "\n",
        "\n",
        "*   fare_amount: can be >0 or 0 (free trip or payment voided?), but not below 0\n",
        "*   tip_amount: logically can't be below 0 (but could be 0)\n",
        "*   trip_distance: can't be <=0 because some distance has to be covered\n",
        "*   total_amount: since it's a sum of multiple components, can't be <= 0"
      ],
      "metadata": {
        "id": "9yLpqeqicxGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_check = ['fare_amount', 'tip_amount', 'trip_distance', 'total_amount']\n",
        "\n",
        "len_before = len(df)\n",
        "\n",
        "for col in cols_to_check:\n",
        "    n_neg = (df[col] < 0).sum()\n",
        "    print(f\"{col}: {n_neg} negative values found\")\n",
        "\n",
        "df = df[(df['fare_amount'] >= 0) & (df['trip_distance'] > 0) & (df['tip_amount'] >= 0) & (df['total_amount'] > 0)]\n",
        "\n",
        "\n",
        "print(f\"\\nA total of {len_before - len(df)} rows removed.\")"
      ],
      "metadata": {
        "id": "AZOCP-YmctoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1"
      ],
      "metadata": {
        "id": "uTLJX8OZc43Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract all trips with trip_distance larger than 50"
      ],
      "metadata": {
        "id": "RsFTWy7SdjFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trips_g_50 = df[df['trip_distance'] > 50].copy().reset_index(drop=True)\n",
        "print(f\"\\n{len(trips_g_50)} rows found.\\n\")\n",
        "print(\"Details for trip_distance greater than 50:\\n\")\n",
        "display(trips_g_50.trip_distance.describe())"
      ],
      "metadata": {
        "id": "RMkUw6JbdhFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2"
      ],
      "metadata": {
        "id": "RZwTpcS3c-HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract all trips where payment_type is missing"
      ],
      "metadata": {
        "id": "8pyqpTXZdlsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trips_nan_pt = df[df['payment_type'].isna()].copy().reset_index(drop=True)\n",
        "print(f\"\\n{len(trips_nan_pt)} rows found.\\n\")\n",
        "print(\"First 5 rows of the subdf:\\n\")\n",
        "display(trips_nan_pt.head().round(2))"
      ],
      "metadata": {
        "id": "wpPYz_AQdgrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Removing {len(df[df['VendorID'].isna()])} rows\")\n",
        "#df = df[df['VendorID'].notna()]"
      ],
      "metadata": {
        "id": "ACFzH8jygKpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3"
      ],
      "metadata": {
        "id": "u85dl5D2dA_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each (PULocationID, DOLocationID) pair, determine the number of trips"
      ],
      "metadata": {
        "id": "dallOrEadmn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: For each (PULocationID, DOLocationID) pair, determine the number of trips\n",
        "\n",
        "trip_counts = df[['PULocationID','DOLocationID']].value_counts().reset_index(name='total_trips_per_route')\n",
        "print(f\"{len(trip_counts)} pairs found.\")\n",
        "print(\"\\nStatistical data about 'total_trips_per_route' distribution:\")\n",
        "display(trip_counts.total_trips_per_route.describe().astype(int))"
      ],
      "metadata": {
        "id": "0p7IyoKQdgXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4"
      ],
      "metadata": {
        "id": "8-4i2eLGdBSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all rows with missing VendorID, passenger_count, store_and_fwd_flag, payment_type in a new DataFrame called bad, and remove those rows from the original DataFrame"
      ],
      "metadata": {
        "id": "VklUI4Esdqfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nan_cols = ['VendorID', 'passenger_count', 'store_and_fwd_flag', 'payment_type']\n",
        "\n",
        "null_indices = df[nan_cols].isna().any(axis=1)\n",
        "\n",
        "print(f\"Number of rows where all specified columns are null: {null_indices.sum()}\")\n",
        "print(\"\\nCheck if rows null in one column are null in others:\")\n",
        "for col in nan_cols:\n",
        "    is_null_in_col = df[col].isna()\n",
        "    # Check if all rows that are null in 'col' are also null in all other columns\n",
        "    check = df.loc[is_null_in_col, [c for c in nan_cols if c != col]].isna().all(axis=1)\n",
        "    print(f\"  - Rows where '{col}' is null are also null in all other specified columns: {check.all()}\")\n",
        "\n",
        "#If the value can be missing in just 1 of the 4 columns:\n",
        "bad = df[null_indices].copy().reset_index(drop=True)\n",
        "df = df.dropna(subset=nan_cols).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n{len(bad)} rows removed from the original dataset.\")\n",
        "print(f\"\\nNow the dataset is {len(df)} rows long.\")"
      ],
      "metadata": {
        "id": "bjBezgCUdgG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5"
      ],
      "metadata": {
        "id": "5vXa2MIedBU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a duration column storing how long each trip has taken (use tpep_pickup_datetime, tpep_dropoff_datetime)"
      ],
      "metadata": {
        "id": "7JA26uYOdrhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = t.time()\n",
        "\n",
        "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
        "\n",
        "df['duration_sec'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds().astype(int)\n",
        "\n",
        "df['duration_str'] = df['duration_sec'].apply(lambda sec: f\"{sec // 86400} days \" if sec // 86400 > 0 else \"\") + \\\n",
        "                                df['duration_sec'].apply(lambda sec: f\"{sec // 3600 % 24} hours \" if sec // 3600 > 0 else \"\") + \\\n",
        "                                df['duration_sec'].apply(lambda sec: f\"{sec // 60 % 60} minutes \") + \\\n",
        "                                df['duration_sec'].apply(lambda sec: f\"{sec % 60} seconds\")\n",
        "df['duration_str'] = df['duration_str'].str.strip()\n",
        "\n",
        "maxdur = df.loc[df['duration_sec'].idxmax()]\n",
        "mindur = df.loc[df['duration_sec'].idxmin()]\n",
        "print(f\"\\nLongest trip: {maxdur.duration_str}\")\n",
        "print(f\"Shortest trip: {mindur.duration_str}\")\n",
        "\n",
        "end = t.time()\n",
        "print(f\"\\n\\n\\n(Execution time: {round(end - start,2)} seconds.)\")"
      ],
      "metadata": {
        "id": "oc1SFoCzigVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6"
      ],
      "metadata": {
        "id": "g7XkeexBdBXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each pickup location, determine how many trips have started there"
      ],
      "metadata": {
        "id": "4Vnf79JQdwB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickup_counts = df['PULocationID'].value_counts().reset_index()\n",
        "pickup_counts.columns = ['PULocationID', 'number_of_trips']\n",
        "\n",
        "print(\"Top 5 most common PULocationID (with the number of trips started there):\\n\")\n",
        "display(pickup_counts.head())\n",
        "print(f\"\\nTotal {len(pickup_counts)} unique pickup locations.\")\n",
        "\n",
        "print(\"\\nStatistical data about the number of trips per pickup location:\")\n",
        "display(pickup_counts.number_of_trips.describe().astype(int))"
      ],
      "metadata": {
        "id": "kLJVb2BhijS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7"
      ],
      "metadata": {
        "id": "4nSN57A3dBZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster the pickup time of the day into 30-minute intervals (e.g., from 02:00 to 02:30)"
      ],
      "metadata": {
        "id": "8zHDYmmldyMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = t.time()\n",
        "\n",
        "df['hour']   = df['hour2']   = df['tpep_pickup_datetime'].dt.hour\n",
        "df['minute'] = df['minute2'] = df['tpep_pickup_datetime'].dt.floor('30min').dt.minute\n",
        "\n",
        "df['hour2'] = np.where(df['minute'] == 30, (df['hour'] + 1) % 24, df['hour']) #[0-23] % 24 -> 0, 24 % 24 -> 1\n",
        "df['minute2'] = np.where(df['minute'] == 30, 0, 30)\n",
        "\n",
        "df['tpep_pickup_time_interval'] = (\n",
        "    df['hour'].map('{:02}'.format) + ':' + df['minute'].map('{:02}'.format) + '-' +\n",
        "    df['hour2'].map('{:02}'.format) + ':' + df['minute2'].map('{:02}'.format)\n",
        ")\n",
        "\n",
        "df.drop(columns=['hour','hour2','minute','minute2'],inplace=True)\n",
        "\n",
        "print('\\nNew column preview:')\n",
        "display(df[['tpep_pickup_datetime','tpep_pickup_time_interval']].head())\n",
        "\n",
        "print('\\nList of all the possible intervals:')\n",
        "display(df['tpep_pickup_time_interval'].unique())\n",
        "\n",
        "end = t.time()\n",
        "print(f\"\\n\\n\\n(Execution time: {round(end - start,2)} seconds.)\")\n",
        "\n",
        "#start = t.time()\n",
        "\n",
        "#df['pickup_time_30min'] = df['tpep_pickup_datetime'].dt.floor('30min')\n",
        "#df['time_interval'] = df['pickup_time_30min'].dt.strftime('%H:%M') + '-' + (df['pickup_time_30min'] + pd.Timedelta(minutes=30)).dt.strftime('%H:%M')\n",
        "\n",
        "#end = t.time()\n",
        "#print(f\"\\nExecution time: {end - start} seconds.\")"
      ],
      "metadata": {
        "id": "fnUvwtcjilzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method ----->\tMean comp. time requested**\n",
        "\n",
        "astype(str).str.zfill(2) ----->\t1×\n",
        "\n",
        ".map('{:02}'.format) ----->\t~0.6×\n",
        "\n",
        ".dt.strftime('%H') su datetime ----->\t3–4×"
      ],
      "metadata": {
        "id": "lNrY-AZKd1Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8"
      ],
      "metadata": {
        "id": "a6JPad0fdIUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each interval, determine the average number of passengers and the average fare amount"
      ],
      "metadata": {
        "id": "ie58p0gHd3VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 9"
      ],
      "metadata": {
        "id": "8GYxLVSGdItj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each payment type and each interval, determine the average fare amount"
      ],
      "metadata": {
        "id": "JOzg_lgqd4FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 10"
      ],
      "metadata": {
        "id": "4I3ez-EkdJB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each payment type, determine the interval when the average fare amount is maximum"
      ],
      "metadata": {
        "id": "GWgYRegod87y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 11"
      ],
      "metadata": {
        "id": "pYu5X35WdJRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each payment type, determine the interval when the overall ratio between the tip and the fare amounts is maximum"
      ],
      "metadata": {
        "id": "4NrPvojdeE78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 12"
      ],
      "metadata": {
        "id": "h54BY8RsdJkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the location with the highest average fare amount"
      ],
      "metadata": {
        "id": "dFrn_RLwd_2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLEASE NOTE:**\n",
        "\n",
        "Since it is not specified whether the location refers to PU or DO, I'll assume the goal is to find where the most expensive trips ***begin***, and therefore I will consider the **PU** locations.\n",
        "\n",
        "The same calculation can, of course, also be done using the DO locations, perhaps to explore the cost of reaching certain areas."
      ],
      "metadata": {
        "id": "T_E6G7g5eA_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 13"
      ],
      "metadata": {
        "id": "iLiWGfD9dJzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a new DataFrame (called common) where, for each pickup location, we keep all trips to the 5 most common destinations (i.e., each pickup location can have different common destinations)"
      ],
      "metadata": {
        "id": "66kGYFUoeGTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 14"
      ],
      "metadata": {
        "id": "YUMpFt-AdKB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the common DataFrame, for each payment type and each interval, determine the average fare amount"
      ],
      "metadata": {
        "id": "FLMzMIgTeJ2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 15"
      ],
      "metadata": {
        "id": "rCZ4m6zNdKR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the difference of the average fare amount computed in the previous point with those computed at point 9"
      ],
      "metadata": {
        "id": "rzdZrcY2eLwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 16"
      ],
      "metadata": {
        "id": "3zVfYxwTdKjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the ratio between the differences computed in the previous point and those computed in point 9 Note: You have to compute a ratio for each pair (payment type, interval)"
      ],
      "metadata": {
        "id": "TeoRzEfMeNiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 17"
      ],
      "metadata": {
        "id": "B9WynSyodZO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build chains of trips. Two trips are consecutive in a chain if:\n",
        "\n",
        "\n",
        "*   They have the same VendorID\n",
        "*   The pickup location of the second trip is also the dropoff location of the first trip\n",
        "*   The pickup time of the second trip is after the dropoff time of the first trip\n",
        "*   The pickup time of the second trip is at most 2 minutes later than the dropoff time of the first trip"
      ],
      "metadata": {
        "id": "9NwqGqsEeSBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REASONING**\n",
        "\n",
        "Task preparation:\n",
        "\n",
        "- reorder the dataset by VendorID, tpep_pickup_datetime, tpep_dropoff_datetime\n",
        "\n",
        "- create a column ‘starting_dropoff’ that contains tpep_dropoff_datetime + 1 second\n",
        "\n",
        "- create a column ‘ending_dropoff’ that contains tpep_dropoff_datetime + 2 minutes\n",
        "\n",
        "- initialize n = 0\n",
        "\n",
        "Then define a main function that:\n",
        "\n",
        "1. creates a column chain initialized to n\n",
        "\n",
        "2. increments n by 1 and takes the first row “i” with chain = 0 and does the following:\n",
        "\n",
        "  - chain of i = n\n",
        "\n",
        "  - crea te a subset of rows whose tpep_pickup_datetime is between the starting_dropoff of i and the ending_dropoff of i and have the same VendorID as i\n",
        "\n",
        "  - check whether in this subset, in the PULocationID column, the DOLocationID of i appears\n",
        "\n",
        "  - if it does not appear: go to step 3\n",
        "\n",
        "  - if it does appear: this row is called “j”, chain of j becomes n, and repeat steps 3A to 3E for j (which thus becomes the new i) but without changing n\n",
        "\n",
        "3. move to the next row with chain = 0 (the new “i”), increment n by 1 and repeat steps 3A to 3E for the new row"
      ],
      "metadata": {
        "id": "Fgx3DxNshY9m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQp9nSy2h3YR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}